{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wikipedia_dataset = load_dataset(\"wikipedia\", \"20220301.en\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from claudegpt.data.datasets.wikipedia import WikipediaDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = WikipediaDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from claudegpt.data.datasets.dataset import Dataset\n",
    "from claudegpt.settings import *\n",
    "\n",
    "import re\n",
    "import pickle \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# from pathlib import Path\n",
    "from datasets import load_dataset, DownloadConfig, concatenate_datasets\n",
    "\n",
    "def save_raw(self) -> None:\n",
    "\t\tif (DATA_FOLDER.joinpath(self.training_part).joinpath(self.name).joinpath(f'train.bin')).exists():\n",
    "\t\t\treturn\n",
    "\t\tDATA_FOLDER.joinpath(self.training_part).joinpath(self.name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\t\tsplit_dataset = self.dataset.train_test_split(test_size = PRETRAINING_VAL_RATIO, shuffle = True)\n",
    "\t\tsplit_dataset['val'] = split_dataset.pop('test')\n",
    "\n",
    "\t\tfor split, documents in split_dataset.items():\n",
    "\n",
    "\t\t\ttotal = 0\n",
    "\t\t\tids = []\n",
    "\n",
    "\t\t\tfor doc in tqdm(documents, desc = f'Saving {self.name} {split} ids'):\n",
    "\n",
    "\t\t\t\tids.append({\n",
    "\t\t\t\t\t'start': total,\n",
    "\t\t\t\t\t'size': len(doc)\n",
    "\t\t\t\t})\n",
    "\n",
    "\t\t\t\ttotal += len(doc)\n",
    "\n",
    "\t\t\twith open(DATA_FOLDER.joinpath(self.training_part).joinpath(self.name).joinpath(f'{split}_ids.pkl'), 'wb') as file:\n",
    "\t\t\t\tpickle.dump(ids, file)\n",
    "\n",
    "\t\t\tbatch_size = 1_024\n",
    "\n",
    "\t\t\twhile batch_size >= len(documents):\n",
    "\t\t\t\tbatch_size //= 2\n",
    "\n",
    "\t\t\tself.size[split] = int(np.sum(len(documents), dtype = np.uint64))\n",
    "\t\t\tpath = DATA_FOLDER.joinpath(self.training_part).joinpath(self.name).joinpath(f'{split}.bin')\n",
    "\t\t\tfile = np.memmap(path, dtype = np.uint16, mode = 'w+', shape = (self.size[split],))\n",
    "\t\t\ti = 0\n",
    "\n",
    "\t\t\tfor batch_i in tqdm(range(batch_size), desc = f'Saving {self.name} {split}'):\n",
    "\n",
    "\t\t\t\tbatch = documents.shard(num_shards = batch_size, index = batch_i, contiguous = True).with_format('numpy')\n",
    "\t\t\t\tfile_batch = np.concatenate(batch['tokens'])\n",
    "\t\t\t\tfile[i:i + len(file_batch)] = file_batch\n",
    "\t\t\t\ti += len(file_batch)\n",
    "\n",
    "\t\t\tfile.flush()\n",
    "\n",
    "\t\twith open(DATA_FOLDER.joinpath(self.training_part).joinpath(self.name).joinpath(f'metadata.pkl'), 'wb') as file:\n",
    "\t\t\tpickle.dump({\n",
    "\t\t\t\t'training_part': self.training_part,\n",
    "\t\t\t\t'name': self.name,\n",
    "\t\t\t\t'size': self.size,\n",
    "\t\t\t\t'multiplier': self.multiplier\n",
    "\t\t\t}, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (DATA_FOLDER.joinpath(self.training_part).joinpath(self.name).joinpath(f'train.bin')).exists():\n",
    "    pass\n",
    "    # return\n",
    "DATA_FOLDER.joinpath(self.training_part).joinpath(self.name).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = self.dataset.train_test_split(test_size = PRETRAINING_VAL_RATIO, shuffle = True)\n",
    "split_dataset['val'] = split_dataset.pop('test')\n",
    "\n",
    "for split, documents in split_dataset.items():\n",
    "\n",
    "    total = 0\n",
    "    ids = []\n",
    "\n",
    "    for doc in tqdm(documents, desc = f'Saving {self.name} {split} ids'):\n",
    "\n",
    "        ids.append({\n",
    "            'start': total,\n",
    "            'size': len(doc)\n",
    "        })\n",
    "\n",
    "        total += len(doc)\n",
    "\n",
    "    with open(DATA_FOLDER.joinpath(self.training_part).joinpath(self.name).joinpath(f'{split}_ids.pkl'), 'wb') as file:\n",
    "        pickle.dump(ids, file)\n",
    "\n",
    "    batch_size = 1_024\n",
    "\n",
    "    while batch_size >= len(documents):\n",
    "        batch_size //= 2\n",
    "\n",
    "    self.size[split] = int(np.sum(len(documents), dtype = np.uint64))\n",
    "    path = DATA_FOLDER.joinpath(self.training_part).joinpath(self.name).joinpath(f'{split}.bin')\n",
    "    file = np.memmap(path, dtype = np.uint16, mode = 'w+', shape = (self.size[split],))\n",
    "    i = 0\n",
    "\n",
    "    for batch_i in tqdm(range(batch_size), desc = f'Saving {self.name} {split}'):\n",
    "\n",
    "        batch = documents.shard(num_shards = batch_size, index = batch_i, contiguous = True).with_format('numpy')\n",
    "        file_batch = batch # np.concatenate(batch['tokens'])\n",
    "        size_batch = len(file_batch[\"text\"])\n",
    "        file[i:i + size_batch] = size_batch\n",
    "        i += len(file_batch)\n",
    "\n",
    "    file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(file_batch[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_FOLDER.joinpath(self.training_part).joinpath(self.name).joinpath(f'metadata.pkl'), 'wb') as file:\n",
    "    pickle.dump({\n",
    "        'training_part': self.training_part,\n",
    "        'name': self.name,\n",
    "        'size': self.size,\n",
    "        'multiplier': self.multiplier\n",
    "    }, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_raw(wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
